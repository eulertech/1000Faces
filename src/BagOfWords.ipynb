{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API requirement\n",
    "# 1. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to stopwords...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from autocorrect import spell\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt', 'stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/spam.csv', encoding=\"ISO-8859-1\");\n",
    "dataset.columns = ['target','sms','v3','v4','v5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sms</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                                sms   v3   v4   v5\n",
       "0    ham  Go until jurong point, crazy.. Available only ...  NaN  NaN  NaN\n",
       "1    ham                      Ok lar... Joking wif u oni...  NaN  NaN  NaN\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...  NaN  NaN  NaN\n",
       "3    ham  U dun say so early hor... U c already then say...  NaN  NaN  NaN\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...  NaN  NaN  NaN"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # remove non-alphabetic characters\n",
    "    textAlphabetic = re.sub('[^A-Za-z]', ' ', text)\n",
    "    # make all words lower case\n",
    "    textLower = textAlphabetic.lower()\n",
    "    # remove stop words\n",
    "    tokenized_text = word_tokenize(textLower)\n",
    "    for word in tokenized_text:\n",
    "        if word in stopwords.words('english'):\n",
    "            tokenized_text.remove(word)\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(len(tokenized_text)):\n",
    "        tokenized_text[i] = stemmer.stem(spell(tokenized_text[i]))\n",
    "\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'smdf', 'aa']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = preprocess(\"this is 0000 2222333 skdf aa2222 07099833605\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'spam']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(dataset.iloc[:, 0]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf vectorizer\n",
    "def stem_tokenize(text):\n",
    "    return [stemmer(i) for i in word_tokenize(text)]\n",
    "\n",
    "tfVectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',\n",
    "                               lowercase=True, token_pattern='[a-zA-Z0-9]+',\n",
    "                               strip_accents='unicode',tokenizer=stem_tokenize)\n",
    "with open('tfidv_vectorizer.pk', 'wb') as fin:\n",
    "    pickle.dump(tfVectorizer, fin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AJ', 'IP', 'RW', 'Ve', 'a', 'abilla', 'abt', 'ac', 'accommod', 'accord', 'aco', 'actin', 'advis', 'aft', 'afternoon', 'ah', 'ahead', 'ahh', 'aid', 'all', 'almost', 'alreadi', 'alright', 'alway', 'amp', 'and', 'aneth', 'anymor', 'apolog', 'appli', 'appoint', 'arabian', 'ard', 'around', 'ask', 'avail', 'award', 'b', 'babe', 'back', 'badli', 'barbi', 'be', 'becom', 'bed', 'beforehand', 'best', 'bit', 'bless', 'bone', 'breather', 'britney', 'brother', 'bu', 'buffet', 'burger', 'burn', 'buy', 'bx', 'c', 'call', 'caller', 'callertun', 'came', 'camera', 'car', 'cash', 'casualti', 'catch', 'cau', 'caught', 'cave', 'chanc', 'charg', 'check', 'cheer', 'child', 'cine', 'cinema', 'claim', 'class', 'clear', 'click', 'close', 'co', 'code', 'cog', 'coin', 'collect', 'colour', 'com', 'come', 'cometh', 'comp', 'complimentari', 'confirm', 'contrat', 'convinc', 'cool', 'copi', 'correct', 'cost', 'could', 'crash', 'crave', 'crazi', 'credit', 'cri', 'cup', 'cuppa', 'custom', 'cut', 'da', 'dark', 'date', 'dauk', 'day', 'decid', 'deliv', 'demic', 'detroit', 'devil', 'di', 'didn', 'digniti', 'dinner', 'divorc', 'done', 'dont', 'doubt', 'download', 'dresser', 'dun', 'e', 'earli', 'earn', 'eat', 'eg', 'egg', 'eh', 'eighth', 'el', 'ela', 'embarrass', 'end', 'endow', 'england', 'enough', 'entiti', 'entri', 'etc', 'eurodisinc', 'even', 'fa', 'factor', 'faint', 'fair', 'fallen', 'fear', 'feel', 'ffffffffff', 'final', 'find', 'fine', 'finish', 'first', 'follow', 'for', 'fore', 'forget', 'forgot', 'four', 'free', 'freemen', 'fri', 'friend', 'fulfil', 'fun', 'fund', 'fyi', 'gati', 'gentleman', 'get', 'girl', 'give', 'go', 'goal', 'goe', 'gon', 'good', 'got', 'gram', 'grant', 'great', 'gt', 'guarante', 'ha', 'hair', 'hairdress', 'half', 'ham', 'happi', 'have', 'havent', 'he', 'hear', 'hee', 'hello', 'help', 'hep', 'hey', 'hi', 'hl', 'hockey', 'hol', 'home', 'hope', 'hor', 'hospita', 'hour', 'housework', 'how', 'http', 'hungri', 'hurt', 'ice', 'il', 'im', 'immunis', 'in', 'inch', 'includ', 'incorrect', 'info', 'invit', 'iq', 'it', 'jacket', 'jackpot', 'jersey', 'job', 'joke', 'jure', 'k', 'kano', 'ken', 'kept', 'kill', 'kl', 'know', 'la', 'lar', 'late', 'later', 'latest', 'lccltd', 'learn', 'left', 'lesson', 'let', 'letter', 'lido', 'like', 'link', 'live', 'load', 'loan', 'long', 'look', 'lor', 'love', 'ls', 'lt', 'luck', 'lunch', 'macedonia', 'machan', 'make', 'mallia', 'man', 'mark', 'matrix', 'may', 'mean', 'meet', 'membership', 'messag', 'minnaminungint', 'miss', 'mix', 'mmmmmm', 'mob', 'mobil', 'moll', 'mom', 'money', 'month', 'more', 'morefrmmob', 'morn', 'moro', 'move', 'msg', 'much', 'multi', 'n', 'na', 'nah', 'name', 'nation', 'naught', 'nd', 'need', 'net', 'network', 'never', 'new', 'news', 'next', 'nice', 'nigeria', 'night', 'nitro', 'nokia', 'not', 'now', 'nummi', 'nurungu', 'odi', 'of', 'offer', 'oh', 'ok', 'onc', 'one', 'oni', 'onli', 'oop', 'oper', 'p', 'pa', 'pack', 'pain', 'part', 'pass', 'password', 'patent', 'pay', 'peopl', 'per', 'perform', 'pick', 'pizza', 'pl', 'place', 'plan', 'plane', 'play', 'plea', 'pleasur', 'pm', 'pobox', 'point', 'pound', 'pour', 'press', 'prize', 'prob', 'promis', 'qjkgighjjgcbl', 'question', 'quick', 'r', 'rain', 'rate', 'reach', 'real', 'realiz', 'realli', 'receiv', 'recent', 'red', 'rememb', 'repli', 'repress', 'request', 'respect', 'rev', 'review', 'reward', 'ride', 'right', 'ringbon', 'rodger', 'room', 'roommat', 'run', 's', 'said', 'same', 'sarcast', 'saturday', 'save', 'saw', 'say', 'scotland', 'search', 'second', 'see', 'seeker', 'seem', 'select', 'send', 'seri', 'seriou', 'servic', 'set', 'shag', 'sheet', 'sherawat', 'shi', 'short', 'show', 'shower', 'shracomorsglsuplt', 'sick', 'simpl', 'sinc', 'situat', 'six', 'slice', 'sm', 'smarter', 'smile', 'smith', 'someon', 'song', 'soon', 'sooner', 'sorri', 'speak', 'special', 'spell', 'spent', 'spiv', 'spoilt', 'st', 'stand', 'start', 'starward', 'std', 'steed', 'still', 'stock', 'stop', 'str', 'stubborn', 'stuff', 'subscript', 'suck', 'sucker', 'sugi', 'sum', 'sunday', 'superman', 'sure', 'sweet', 't', 'ta', 'take', 'talk', 'tb', 'tea', 'team', 'tell', 'telugu', 'test', 'text', 'thank', 'that', 'the', 'thi', 'think', 'tho', 'though', 'thought', 'ticket', 'till', 'time', 'tit', 'today', 'tomorrow', 'tonight', 'too', 'took', 'trav', 'treat', 'tri', 'trip', 'troubl', 'turn', 'txt', 'tyler', 'u', 'uk', 'updat', 'ur', 'urgent', 'url', 'us', 'use', 'usher', 'usual', 'v', 'vagu', 'valid', 'valu', 'valuabl', 'verifi', 'vesta', 'w', 'wa', 'wah', 'wait', 'wale', 'wan', 'want', 'wap', 'wat', 'watch', 'watt', 'way', 'weak', 'week', 'weekend', 'weli', 'well', 'wen', 'wet', 'whole', 'win', 'wing', 'winner', 'wk', 'won', 'wonder', 'wont', 'word', 'work', 'world', 'worri', 'wow', 'wun', 'www', 'x', 'xml', 'xuhui', 'xx', 'xxx', 'xxxmobilemovieclub', 'ye', 'yeah', 'year', 'yesterday', 'yo', 'you', 'your', 'yup']\n"
     ]
    }
   ],
   "source": [
    "# creating the feature matrix using bag of words\n",
    "# vectorizer = CountVectorizer(max_features=10000,decode_error=\"replace\", \n",
    "#                              stop_words='english', min_df=2, max_df=0.7)\n",
    "vectorizer = CountVectorizer(analyzer=preprocess)\n",
    "# vectorizer = TfidfVectorizer()\n",
    "cvf = vectorizer.fit(dataset.iloc[0:100,]['sms_processed'])\n",
    "X = cvf.transform(dataset.iloc[0:100,]['sms_processed']).toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "y = le.fit_transform(dataset.iloc[0:100, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transformer.pkl','wb') as f1:\n",
    "    pickle.dump(cvf, f1)\n",
    "\n",
    "with open(\"simpleModel.pkl\",\"wb\") as f2:\n",
    "    pickle.dump(classifier, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_transformer.pkl','wb') as f1:\n",
    "    pickle.dump(tfVectorizer, f1)\n",
    "with open('tfidf_transformer.pkl','rb') as f1:\n",
    "    xx = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = cvf.transform([\"go jure point crazi avail in sugi n great world\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testScore = classifier.predict(transformed)\n",
    "testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,modelPath = \"simpleModel.pkl\", vectorizer = 'transformer.pkl'):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "    # preprocess\n",
    "    text_preprocessed = preprocess(text)\n",
    "    with open(vectorizer, 'rb') as f1:\n",
    "        vectorizer = pickle.load(f1)\n",
    "    with open(modelPath, \"rb\") as f2:\n",
    "        clf = pickle.load(f2)\n",
    "    score = clf.predict(vectorizer.transform([text]).toarray())\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"go jure point crazi avail in sugi n great world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
